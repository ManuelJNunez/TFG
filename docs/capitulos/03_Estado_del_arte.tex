\chapter{Estado del arte}

\section{Ingeniería del Software}

En esta sección voy a describir las herramientas que se usarán en este proyecto así como la justificación del uso de cada una. Todas están enfocadas para lograr las mejores prácticas de la metodología DevOps.

\subsection{Lenguaje elegido}

El lenguaje que he elegido para la realización de este proyecto ha sido \textit{Python}, por ser muy fácil de aprender y además tiene un gran número de bibliotecas para ciencia de datos.\\

Es cierto también que \textit{Python} es un lenguaje interpretado y bastante lento si tenemos el tiempo de ejecución, por eso la mayoría de las bibliotecas de cálculo, álgebra o ciencia de datos que realizan muchos cálculos están implementadas en otros lenguajes como \textit{C} o \textit{CUDA}. Existen alternativas muy buenas como por ejemplo \textit{Julia}, \textit{Go} o \textit{Rust} que son lenguajes compilados y con rendimiento muy parecido a \textit{C} o \textit{C++}, pero no tienen un ecosistema de bibliotecas para ciencias de datos tan grande como el de \textit{Python}. Otra alternativa interesante sería \textit{Node}, una runtime para \textit{JavaScript} (con la cual podríamos usar el mismo lenguaje para programar el \textit{front-end} y el \textit{back-end}) muy potente, pero pasa lo mismo que con lenguajes anteriores. El ecosistema de \textit{Node} está más bien enfocado a desarrollo de microservicios.

\subsection{Gestor de dependencias}

El gestor de dependencias es una herramienta muy útil hoy en día para poder manejar las dependencias de una aplicación o un proyecto de forma sencilla, usando las órdenes del mismo. Además nos permite instalar todas las bibliotecas usando únicamente una orden. Esto claramente ayuda a mantener la reproducibilidad de la aplicación ya que facilita bastante la ejecución de la misma aplicación en otra máquina (como por ejemplo en el entorno de Integración continua, en la máquina de entrenamiento de modelos o en la que desplegaremos un microservicio). En Python, los dos principales gestores de dependencias son \enquote{Pipenv} y \enquote{Poetry}.Ambas herramientas gestionan por debajo un entorno virtual aislado que contiene las dependencias instaladas.\newline

Algo que también hay que tener en cuenta es la versión del interprete que estemos usando, el usuario final debe de saber en qué versión o versiones del intérprete puede ejecutar el software sin problemas. Esto con las dos herramientas anteriores no es problema, pues en el mismo fichero de dependencias se puede especificar la versión o versiones del intérprete necesaria para poder ejecutar el software sin problemas.

\subsubsection*{Pipenv}

Pipenv se define como una herramienta que apunta a traer todo lo mejor del mundo del empaquetado al mundo Python \cite{pipenv}. Usa un fichero con sintaxis TOML para registrar las dependencias cuyo nombre es Pipfile.

\subsubsection*{Poetry}

Poetry es una herramienta cuya popularidad está creciendo un montón actualmente en la comunidad de Python. Es utilizada únicamente para manejar las dependencias de cualquier proyecto de forma muy sencilla. Además, cuenta con una documentación muy buena y clara \cite{poetry}.

\subsubsection*{Comparación de ambas herramientas}

Para comparar ambas herramientas para el manejo de dependencias, voy a utilizar el tiempo que tardan en instalar las librerías usando el fichero \textit{lock}. Las librerías que he usado como dependencias han sido \textit{PyTorch} y \textit{FastAPI} y de dependencias de desarrollo \textit{pylint} y \textit{pytest}. En la siguiente tabla se pueden ver los resultados:

\begin{table}[H]
\begin{tabular}{|c|c|c|}
\hline
                     & \textbf{Pipenv (s)} & \textbf{Poetry (s)} \\ \hline
\textbf{Ejecución 1} & 31,938          & 22,009          \\ \hline
\textbf{Ejecución 2} & 38,528          & 23,068          \\ \hline
\textbf{Ejecución 3} & 33,63           & 23,111          \\ \hline
\textbf{Ejecución 4} & 35,452          & 21,407          \\ \hline
\textbf{Ejecución 5} & 33,406          & 20,901          \\ \hline\hline
\textbf{Media}       & 34,59           & 22,10           \\ \hline
\textbf{Std. Dev.}       & 2,53           & 0,985           \\ \hline
\end{tabular}
\centering
\caption{Tiempo (medido en segundos) que tarda cada herramienta en instalar las dependencias anteriores.}
\label{tab:poetryvspipenv}
\end{table}

Como se puede ver en la Tabla \ref{tab:poetryvspipenv}, Poetry es siempre más rápido que Pipenv, por lo tanto usaré Poetry para manejar las dependencias del proyecto. Además me gustaría añadir que en el momento del testeo, intenté instalar la librería \textit{Black}, pero Pipenv me dió problemas debido a que a veces no es capaz de resolver bien las dependencias, cosa que con Poetry no pasó. He hecho el test de tiempo debido a que necesito que se instalen las dependencias lo más rápido posible para que la Integración Continua dure lo menos posible en ejecutarse y para que se despliegue rápido la aplicación, ya que la instalación de dependencias debe realizarse en el menor tiempo posible para optimizar los pipelines de Integración Continua y de Despliegue Continuo o hacer que alguien que intente ejecutar nuestro proyecto en otra máquina, lo consiga lo antes posible.

\subsection{Gestor de tareas}

Para seguir ayudando con la reproducibilidad del proyecto, necesitamos un gestor que contenga implementadas (con órdenes \textit{shell} o programadas en un lenguaje de alto nivel) las tareas necesarias para construir, testear y ejecutar el código fuente. Esto nos permite que nuestro proyecto se pueda ejecutar en otra máquina de forma sencilla, ejecutando una sola orden que ya lo hace todo por nosotros porque ha sido anteriormente programada. \textit{Pipenv} trae soporte para registrar tareas que lanzan una orden shell (o varias órdenes concatenándolas con el doble ampersand). Para \textit{Poetry} existen dos herramientas que nos permiten también registrar tareas que lancen una orden shell (como \textit{poe the poet} y \textit{taskipy}) y que se definen en el propio \path{pyproject.toml}. Pero esta vez he preferido usar \textit{invoke}, ya que permite implementar las tareas en \textit{Python}.\\

Las ventajas que habrían al implementar las tareas en \textit{Python} en lugar de en \textit{bash} serían las siguientes:

\begin{itemize}
    \item La más obvia es portabilidad. Los scripts de \textit{Python} se pueden ejecutar en todas las máquinas siempre y cuando tengan un intérprete instalado o construido. Los scripts de \textit{bash} solo se pueden ejecutar en sistemas \textit{UNIX}, no se pueden ejecutar en \textit{Windows} por ejemplo de forma nativa.
    \item Otra ventaja clara es que el lenguaje \textit{Python} tiene una sintaxis más clara y sencilla que los scripts de \textit{bash}, haciendo más fácil el mantenimiento de los scripts \textit{Python}.
\end{itemize}

Para lanzar una tarea, es tan fácil como escribir en una consola de comandos \textit{poetry run invoke $<taskname>$} (o \textit{invoke $<taskname>$} si no estamos usando un \textit{virtual environment}). Algunos ejemplos de tareas implementadas usando esta herramienta pueden ser lo siguientes:

\begin{lstlisting}[language=Python, caption={Ejemplo de tareas implementadas con \textit{invoke}.}, captionpos=b]
@task
def test(c):
    c.run("coverage run --source=src/ml -m pytest", pty=True)

@task
def train(c, all=False):
    experiments_path = Path("src/experiments")
    config = dotenv_values(".env")
    train_files = []
    train = True

    for child in experiments_path.iterdir():
        if child.suffix in [".yml", ".yaml"]:
            train_files.append(child)

    for file in train_files:
        if not all:
            train = False
            reply = input(
                f"Would you like to run the experiment defined in {str(file)}? [Y/n] "
            )

            if reply.lower() in ["1", "y", "yes", "true", "yeah"]:
                train = True

        if train:
            c.run(f"snapper-ml --config_file={str(file)}", pty=True, env=config)
            c.run("rm -rf artifacts/")
\end{lstlisting}

La primera tarea definida lo que hace es simplemente ejecutar los tests (el argumento \textit{pty} sirve para que muestre los colores en la salida por pantalla). La segunda lo que hace es buscar los ficheros con extensión \path{.yaml} o \path{.yml} y preguntar al usuario por cada experimento, si no ha usado el argumento \textit{--all}, si quiere ejecutarlo (sobre los experimentos y \textit{SnapperML} se entrará en detalle más adelante en este documento).

\subsection{Plataforma de CI/CD}\label{subsec:cicdplatforms}

La finalidad de esta plataforma es ejecutar los tests y realizar el despliegue cada vez que se produzca un incorporación de código al repositorio de GitHub. Se usará para controlar los cambios en el código. En la CI, lo que hace es ejecutar los tests para comprobar que todo funcione bien. Si la CI tiene éxito, los cambios se aprueban y se combinan en el repositorio compartido y se despliega la aplicación en la máquina destino. Esto sería así en DevOps, pero en MLOps tenemos que entrenar los modelos antes de desplegarlos, por lo cuál el workflow se hace un poco más complejo.

\begin{figure}[h]
	\includegraphics[scale=0.4]{imagenes/03_Estado_del_arte/ci-cd-flow.png}
	\centering
	\caption{Flujo de trabajo CI/CD. \cite{cicd}}
\end{figure}

Casi todas las plataformas de CI/CD tienen un plan de pago pero también tienen un plan gratuito. Mi criterio al seleccionar una plataforma de CI/CD concreta es gastar el menor dinero posible. Por ejemplo Travis, una plataforma que antes era gratuita y es muy rápida, otorgan ahora 10.000 créditos de los cuales, debido al uso que le di en una asignatura, solo me quedan 1650, por lo que esta plataforma queda descartada. En el caso de CircleCI, otorga 2.500 créditos gratis al mes, lo cual limitaría el número de subidas de código que puedo realizar en un mes y puede ser que en algún momento se me olvide esto y me pase del plan gratuito y no pueda volver a ejecutar el pipeline de CI/CD en lo que quede de mes.\newline

Existen algunas plataformas gratuitas como por ejemplo Shippable, pero es demasiado lenta. GitHub Actions también es gratuita para repositorios públicos, como es el caso por lo que estaría interesante usarlo para comprobaciones básicas del repositorio, como por ejemplo comprobar la presencia de archivos importantes del mismo o comprobar la ortografía del README.\newline

Una plataforma a la que seguro le daré uso será Jenkins, tal vez una de las plataformas de CI/CD más populares implementada en Java. Esta plataforma es muy configurable y además dispone de un montón de plugins desarrollados por la comunidad que añaden un montón de utilidades. Para hacerlo funcionar, tenemos que instalarlo en un computador, por lo que sería necesario una instancia en alguna plataforma cloud para tenerlo disponible el mayor tiempo posible o incluso se podría ejecutar en una Raspberry Pi, lo único que necesitamos para que funcione es una Máquina Virtual Java. En mi caso tengo 150\$ en la plataforma AWSEducate, por lo que podría desplegar una instancia con Jenkins estando disponible el mayor tiempo posible sin problemas.\newline

En conclusión, las plataformas que se usarán serán Jenkins aprovechando el plan gratuito y el plan de educación de EC2 y además usaré GitHub Actions porque es completamente gratuito para proyectos de software libre como el mío. Ambas plataformas de CI/CD serán usadas con fines distintos.\\

Todas las plataformas anteriores disponen de un fichero de configuración que se almacena en el repositorio en el que hay que describir la secuencia de órdenes para poder ejecutar los tests o hacer el despliegue, además de especificar el contenedor en el que ese ejecuta, versiones del lenguaje, etc.

\subsection{Plataforma de análisis de código}

El objetivo de usar estas plataformas es para evaluar la calidad de nuestro código. Con calidad del código me refiero a que siga las \textit{code conventions} de \textit{Python} (o de cualquier otro lenguaje que elijamos para realizar nuestro proyecto), las cuales ayudan a mejorar la legibilidad y mantenibilidad del código. En adición a lo anterior, permiten registrar el resultado de los tests de cobertura los cuales nos dicen qué porcentaje de código es alcanzado por nuestro tests (cuanto más alto mejor). Y por último pero no menos importante, analizan el porcentaje de código duplicado con el fin de evitar malas prácticas.\newline

El modo de funciona de estas herramientas es conectándolas a nuestro repositorio de \textit{GitHub} (no soportan otras forjas de código) y a partir de ahí, cada vez que se realice un \textit{push}, se lanzan automáticamente los análisis.\newline

Existen dos alternativas muy conocidas de plataformas de análisis de calidad de código, una es \textit{Codacy} \cite{codacy} y la otra es \textit{Code Climate} \cite{codeclimate}. Ambas son muy parecidas, pero me he quedado con \textit{Codacy} porque usa unas reglas más exigentes y me detecta más issues y ninguno es falso positivo.\newline

A parte de la plataforma de análisis de código, también sería interesante comprobar el cumplimiento de las \textit{code conventions} de \textit{Python} cada vez que se suba código al repositorio utilizando la plataforma de CI/CD. Para ello existen herramientas como \textit{Pylint} (para comprobar que el código sigue las reglas y además detecta errores lógicos) la cual es oficial y \textit{Black} (para cuidar el formato del código y corregirlo). 

\subsection{Framework para MLOps}

Lo que necesito de un framework para MLOps es que registre los experimentos realizados (hiperparámetros usados y resultados obtenidos) y además nos facilite la reproducibilidad de los mismos, es decir, que se obtengan los mismos resultados independientemente de la máquina/entorno en el que se ejecuten.\newline

Existe un framework llamado SnapperML que facilita todo lo anterior. Permite almacenar los hiperparámetros de los modelos en formato YAML, además de tener un control de la estocasticidad (recoge información sobre la semilla utilizada para los diferentes generadores de números aleatorios). También permite el \textit{tracking} de experimentos usando \textit{MLFlow}, una herramienta que permite registrar métricas, hiperparámetros y artefactos y nos permite acceder a ellos a través de una interfaz web bastante intuitiva. Además de todo lo anterior, usa una librería llamada \textit{Optuna} para la optimización de hiperparámetros y \textit{Ray} para la ejecución en cluster \cite{snapperml}.\newline

Como se puede ver, este framework es todo lo que necesito para conseguir la reproducibilidad de los modelos de Machine Learning, pues la reproducibilidad del software ya está conseguida gracias a los gestores de dependencias y de tareas.

\subsection{Aprovisionamiento de Infraestructura Virtual}

Otro detalle que no se nos puede olvidar, es la reproducibilidad de la infraestructura para que la misma pueda ser creada y aprovisionada en otras cuentas de la misma plataforma cloud de forma rápida y fácil. ¿Para qué queremos aprovisionar una infraestructura? En este caso, es necesaria por ejemplo para aprovisionar una instancia de EC2 con \textit{Jenkins}.\newline

Para esto existen dos alternativas muy populares a día de hoy: \textit{Pulumi} \cite{pulumi} y \textit{Terraform} \cite{Terraform}. Ambas nos dejan describir la Infraestructura como Código (IaC), lo que nos facilita un montón la reproducibilidad. Las diferencias entre ambos son las siguientes:

\begin{itemize}
	\item \textbf{Lenguaje de programación}: \textit{Terraform} usa Hashicorp Configuration Language (HCL), un lenguaje propio declarativo para poder describir nuestra infraestructura. Con \textit{Pulumi}, se puede usar un lenguaje de programación de propósito general (\textit{TypeScript}, \textit{JavaScript}, \textit{Python}, \textit{Go} o \textit{C\#}). Usar un lenguaje de propósito general puede ser más cómodo que usar uno declarativo, pues por ejemplo nos permite usar estructuras condicionales si llega a ser necesario.
	\item \textbf{Documentación}: a día de hoy \textit{Terraform} tiene una documentación más completa que \textit{Pulumi}. Es por eso que a lo mejor también tiene una mayor comunidad que \textit{Pulumi}.
	\item \textbf{Modularidad}: \textit{Terraform} presenta una mejor modularidad que \textit{Pulumi}, esto es gracias a que presenta componentes que son perfectamente reusables.
\end{itemize}

En conclusión, ambas herramientas están muy bien diseñadas, cada una tiene sus pros y sus contras. No obstante, he decidido usar \textit{Terraform} para aprovisionar las máquinas necesarias para este proyecto debido a que tener una buena documentación es algo que debería ser esencial hoy en día para poder aprender a usar dicha herramienta sin problemas o consultarla cuando algo falle.

\subsection{Contenedores}

Además de lo dicho en el capítulo de introducción, los contenedores no se ejecutan sobre un hipervisor que funcionan sobre el Sistema Operativo Host, como sí hacen las máquinas virtuales, sino que se ejecutan como un proceso más y comparten el Kernel. Esto hace que las imágenes de los contenedores sean ligeras y además no degrade el rendimiento de la aplicación al no necesitar una capa de abstracción sobre el Sistema Operativo Host.\newline

Otra gran característica de esta tecnología es que se puedan desarrollar nuevas actualizaciones del propio contenedor de forma fácil, pues utilizan por debajo un sistema de versionamiento.

\begin{figure}[H]
	\includegraphics[scale=0.3]{imagenes/03_Estado_del_arte/containers.png}
	\centering
	\caption{Diferencia entre un contenedor y una Máquina Virtual.}
\end{figure}

Entre los \textit{container engines} más populares tenemos \textit{Docker} \cite{docker} y \textit{Podman} \cite{podman}. Recientemente, \textit{Docker} ha sido deprecado en \textit{Kubernetes} (una tecnología para orquestar contenedores ejecutándose en un cluster) debido a problemas de seguridad que presenta su diseño (usa un demonio para funcionar y ejecuta los contenedores con permisos root). Además, \textit{Docker} tiene un montón de servicios, como por ejemplo, el de gestión de volúmenes, de redes y \textit{image builder} que \textit{Kubernetes} no utiliza (solo precisa del \textit{container runtime}) y por tanto se estaría desperdiciando recursos computacionales. \textit{Podman} arregla los problemas que presenta \textit{Docker}, pues no necesita de un demonio para ejecutarse y es \textit{rootless} lo que lo hace un \textit{engine} más seguro. Al no ejecutar los contenedores como \textit{root}, no existe el riesgo de que alguien tenga acceso al servicio de contenedores y pueda ejecutarlos como usuario root y hacer estragos en nuestro computador. Lo que hace es crear un directorio en el directorio \textit{home} con información sobre las imágenes y contenedores que tiene un determinado usuario \cite{podmanvsdocker}.\newline

Por añadir un problema más de seguridad para \textit{Docker} es que una infiltración en un servicio de \textit{Docker} es más peligrosa que una escalada de privilegios. Pues, si tenemos permisos de administrador nuestras acciones se registran en los logs del sistema, pero si accedemos al servicio de \textit{Docker} y realizamos cualquier método para escalar privilegios y nos deshacemos de él luego, no quedaría huella de nuestros actos malignos \cite{podmanvsdocker}.

\section{Modelos de Machine Learning y Deep Learning}

En esta sección se describirán los modelos de Machine Learning o Deep Learning que se planean usar en los experimentos sobre el dataset. Debemos  tener en cuenta que nos estamos enfrentando a un problema de clasificación, es decir, tenemos que entrenar modelos que intenten predecir un valor discreto \{0, 1\}\footnote{También se pueden entrenar modelos de Machine Learning para predecir un valor continuo, como por ejemplo una probabilidad o un valor monetario pero en este trabajo solo se considera el caso de salida discreta.}.

\subsection{Concepto básicos}

Algunos de los conceptos básicos sobre Machine Learning o Deep Learning son los siguientes \cite{mlcoursera}:

\begin{itemize}
	\item \textbf{Dataset}: conjunto de datos que disponemos para entrenar el modelo predictivo. Este dataset se divide en 3 subconjuntos disjuntos, para entrenenar, validar y evaluar el modelo.
	\item \textbf{Training Dataset}: contiene los datos que usaremos para ajustar el modelo.
	\item \textbf{Validation Dataset}: los datos contenidos en este dataset se utilizan para la selección de hiperparámetros más convenientes para entrenar el modelo.
	\item \textbf{Test Dataset}: se utiliza para evaluar la potencia predictiva del modelo.
	\item \textbf{Alto sesgo (high bias)}: esto se produce cuando el poder predictivo del modelo en el conjunto de entrenamiento es malo debido a un sesgo en los datos o porque no se han elegido los hiperparámetros adecuados (por ejemplo por haber elegido un valor demasiado bajo de \textit{learning rate} y número de épocas\footnote{Estos conceptos están explicados más adelante en este capítulo.}).
	\item \textbf{Alta variabilidad (high variability)}: esto se produce cuando el modelo se comporta muy bien en el conjunto de entrenamiento, pero funciona muy mal en el conjunto de test o validación (hace predicciones muy malas). Esto es consecuencia del \textit{overfitting}, que se produce porque el modelo se sobreajusta al conjunto de entrenamiento y por tanto no rinde bien en el conjunto de test. Esto suele pasar porque no tenemos datos suficientes para hacer una predicción en condiciones, porque hemos usado un modelo muy complejo, porque los datos no son representativos, no se han barajado bien o balanceado correctamente, etc.
\end{itemize}

Siempre el conjunto de entrenamiento debe ser mayor que los demás, ya que usar un número muy grande de datos hace que el modelo generado del entrenamiento sea mejor. En los capítulos posteriores explicaré los modelos estado del arte que usaré y los mejores algoritmos de entrenamiento de modelos actuales. También hablaré sobre como podemos evitar el overfitting usando un método muy sencillo llamado regularización.

\subsection{Redes neuronales}

Las redes neuronales son un modelo muy usado para aproximar funciones y en muchos problemas más. Fue inventada en los años 80, pero no eran muy usadas las redes profundas debido a que no había computadores tan potentes como los de ahora. Gracias a las capacidades de computación modernas, se han podido hacer Redes Neuronales más profundas que ofrecen mejores resultados \cite{patterson_gibson}.\newline

Este modelo de aprendizaje automático se puede usar tanto para regresión lineal (cuando el valor que queremos predecir es continuo) como para regresión logística o clasificación (intentamos predecir un valor discreto que representa la clase a la que pertenece la instancia del dataset). Vamos a pasar a ver cómo funciona este modelo tan popular, empezando por un modelo muy básico.

\begin{figure}[H]
	\includegraphics[scale=0.25]{imagenes/03_Estado_del_arte/simplenn.png}
	\centering
	\caption{Red neuronal básica \cite{ng}.}
	\label{fig:simlestnn}
\end{figure}

En la Figura \ref{fig:simlestnn} se puede observar una Red Neuronal muy simple, de hecho este caso específico de red es lo que se llama regresión lineal o logística (dependiendo del rango que tome el valor que intentamos predecir). Una neurona no es más que una función cuya salida es $h_{W,b}(x)=f(W^Tx+b)$, el parámetro $b$ corresponde a las unidades de \textit{bias} y debe ser aprendido al igual que $W$ que representa los pesos. La función $f$ es la función de activación, las más comunes son sigmoide (\ref{eq:sigmoid}), tangente hiperbólica (\ref{eq:tanh}) y RELU (\ref{eq:RELU}). El vector $x$ es el vector de datos, es decir, contiene el dataset. Expresado de otra forma más sencilla, la salida es la siguiente:\newline

$$h_{W,b}=f(W_1 x_1+ W_2 x_2 + W_3 x_3 + b)$$

Las funciones de activación de las que hablé pueden ser las siguientes:

\begin{equation}
	f(x) = \frac{1}{1+e^{-x}}
	\label{eq:sigmoid}
\end{equation}

\begin{equation}
	f(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
	\label{eq:tanh}
\end{equation}

\begin{equation}
	f(x) = \max (0, x)
	\label{eq:RELU}
\end{equation}

Con el modelo anterior podríamos predecir un valor o la probabilidad de que una instancia del Dataset pertenezca a una clase u otra (usando por ejemplo la sigmoide que devuelve un valor entre 0 y 1). Ahora que sabemos lo básico, podemos pasar a explicar otra un poco más compleja para comprender como funciona una red neuronal. \newline

\begin{figure}[H]
	\includegraphics[scale=0.2]{imagenes/03_Estado_del_arte/complexnn.png}
	\centering
	\caption{Red neuronal un poco más compleja \cite{ng}.}
	\label{fig:complexnn}
\end{figure}

En la Figura \ref{fig:complexnn} podemos ver una Red Neuronal más compleja que la anterior la cual nos permite aproximar funciones también más complejas. Tanto en esta como en la anterior podemos observar que el tamaño de la entrada en la red es el tamaño de una instancia del dataset + 1 (la unidad de \textit{bias}). El tamaño del output, en un problema de clasificación depende del número de clases que tengamos, en este caso sirve para 2. En este caso, para realizar una predicción deberíamos usar el algoritmo de propagación hacia delante:

\begin{align*}
&a^{(1)} = x^{(i)} \\
&z^{(2)} = W^{(1)}a^{(1)} + b^{(1)} \\
&a^{(2)} = f(z^{(2)}) \\
&z^{(3)} = W^{(2)}a^{(2)} + b^{(2)} \\
&a^{(3)} = f(z^{(3)}) \\
&z^{(4)} = W^{(3)}a^{(3)} + b^{(3)} \\
&h_{W,b}(x) = a^{(4)} = f(z^{(4)})\\
\end{align*}

Una vez obtenido el resultado del algoritmo de propagación hacia delante, podemos obtener la probabilidad de que la instancia pertenezca a cada clase usando \textbf{softmax}:

\begin{align*}
\sigma (z)_j = \frac{e^{z_j}}{\sum^{K}_{k=1} e^{z_k}}
\end{align*}

\subsubsection{Descenso del gradiente}\label{subsub:gradientdescent}

Anteriormente se ha explicado cómo funciona una red neuronal para predecir, pero ¿Cómo se entrenan para tal objetivo? Primero tenemos que especificar la función que queremos optimizar o dicho de otras palabras, la función que calcula el error en las predicciones y que queremos optimizar. Puede ser el Mean Squared Error (MSE):

\begin{align*}
	&J(W,b;x^{(i)},y^{(i)}) = \frac{1}{m} \sum^m_{i=1} (h_{W,b}(x^{(i)}) - y^{(i)} )^2
\end{align*}

En este caso, el error se calcula como la diferencia entre el valor predecido y el valor correcto. Otra función que podemos usar es la de Cross Entropy (CE):

\begin{align*}
	&J(W,b;x^{(i)},y^{(i)}) = - \frac{1}{m}\left(\sum^m_{i=1} y^{(i)} \cdot \log{h_{W,b}(x^{(i)}})\right)
\end{align*}

El caso específica de CE para dos clases se calcula así:

\begin{align*}
	&J(W,b;x^{(i)},y^{(i)}) = - \frac{1}{m} \left( \sum^m_{i=1} y^{(i)} \cdot \log (h_{W,b}(x^{(i)})) + (1 - y^{(i)}) \cdot \log (1 - h_{W,b}(x^{(i)})) \right)
\end{align*}

Una vez elegida la función de pérdida, debemos optimizarla usando el algoritmo llamado \textit{descenso del gradiente}. Dicho algoritmo actualiza los pesos en cada iteración de la siguiente forma en los modelos básicos de regresión (la red neuronal más básica):

\begin{align*}
	&W_j = W_j - \alpha \frac{\partial}{\partial W_j} J(W,b)\\
	&b = b - \alpha \frac{\partial}{\partial b} J(W,b)\\
\end{align*}

El parámetro $\alpha$ representa el ratio de aprendizaje y es otro parámetro que debemos elegir usando el conjunto de validación. Si es muy alto, el aprendizaje puede fallar y si es muy bajo el entrenamiento tarda un montón en finalizar. Sin embargo, calcular las derivadas parciales en una red neuronal con más capas es un poco más complejo:

\begin{align*}
	&W^{(l)}_{ij} = W^{(l)}_{ij} - \alpha \frac{\partial}{\partial W^{(l)}_{ij}} J(W,b)\\
	&b^{(l)}_i = b^{(l)}_i - \alpha \frac{\partial}{\partial b^{(l)}_i} J(W,b)\\
\end{align*}

Para calcular las derivadas parciales de forma eficiente, se usa el algoritmo de \textit{propagación hacia detrás}:\\

\begin{algorithm}[H]
	\SetAlgoLined
	
	Calcular $a^{(l)}$ $\forall l \in \{2, ..., L\}$ usando el algoritmo de \textit{propagación hacia delante}
	
	Para la capa de salida, calculamos: $\delta^{(L)} = \frac{\partial}{\partial z^{(L)}} \frac{1}{2} || y - h_{W,b}(x) ||^2 = (a^{(L)}- y) \odot f'(z^{(L)})$
	
	\For{$l \in \{L-1, ..., 2\}$}{
		\nonl $\delta^{(l)} = ((W^{(l)})^T \delta^{(l+1)}) \odot f'(z^{(l)})$
	}
	
	Calcular las derivadas parciales:\\
	\nonl $\frac{\partial}{\partial W^{(l)}} J(W,b) = \delta^{(l+1)}(a^{(l)})^T$\\
	\nonl $\frac{\partial}{\partial b^{(l)}} J(W,b) = \delta^{(l+1)}$
	
	\caption{Backpropagation Algorithm}
\end{algorithm}

\subsection{Algoritmos de aprendizaje}\label{subsec:learningalgorithms}

\subsubsection{Descenso del gradiente estocástico}

Con un dataset muy grande, como es el caso, el descenso del gradiente normal tarda mucho en converger a una solución óptima. En ese caso, puede resultar beneficioso el uso del Stochastic Gradient Descent (SGD).\newline

Este algoritmo consiste en tomar un subconjunto (mini-batch) de datos aleatorio y mucho más pequeño que el propio dataset en cada iteración. Después de haber selecciondo aleatoriamente dicho subconjunto, realizamos una iteración de Gradient Descent (GD) sobre el mini-batch.

\subsubsection{Adaptative Moment Estimation (Adam)}

Adam hace uso de \textit{momentum}, un método de actualiza los pesos no solo teniendo en cuenta el gradiente actual, sino también todos los anteriores. Esto hace que el ajuste sea más suave.

\begin{align}
	&V_{dW} \leftarrow \beta \cdot V_{dW} + (1 - \beta) \frac{\partial}{\partial W} J(W,b)\\
	&V_{db} \leftarrow \beta \cdot V_{db} + (1 - \beta) \frac{\partial}{\partial b} J(W,b)\\
	&W \leftarrow W - \alpha V_{dW}
	&b \leftarrow b - \alpha V_{db}
\end{align}

Y también usa \textit{RMSprop}, un método parecido a \textit{momento} pero la diferencia es que usa el segundo momento del gradiente para hacer el ajuste más suave.

\begin{align}
	&S_{dW} \leftarrow \beta_2 \cdot S_{dW} + (1 - \beta_2) \left( \frac{\partial}{\partial W} J(W,b) \right)^2\\
	&S_{dW} \leftarrow \beta_2 \cdot S_{dW} + (1 - \beta_2) \left( \frac{\partial}{\partial b} J(W,b) \right)^2\\
	&W \leftarrow W - \alpha \frac{\frac{\partial}{\partial W} J(W,b)}{\sqrt{S_{dW} + \epsilon}}
	&b \leftarrow b - \alpha \frac{\frac{\partial}{\partial b} J(W,b)}{\sqrt{S_{db} + \epsilon}}
\end{align}

Adam hace uso de los métodos anteriores para conseguir un ajuste más suave que el SGD, llegando antes a una solución óptima local y mejor en muchos casos.

\begin{align}
	&t \leftarrow iteration\_number\\
	&V_{dW} \leftarrow \beta_1 \cdot V_{dW} + (1 - \beta_1) \frac{\partial}{\partial W} J(W,b)\\
	&V_{db} \leftarrow \beta_1 \cdot V_{db} + (1 - \beta_1) \frac{\partial}{\partial b} J(W,b)\\
	&S_{dW} \leftarrow \beta_2 \cdot S_{dW} + (1 - \beta_2) \left( \frac{\partial}{\partial W} J(W,b) \right)^2\\
	&S_{dW} \leftarrow \beta_2 \cdot S_{dW} + (1 - \beta_2) \left( \frac{\partial}{\partial b} J(W,b) \right)^2\\
	&V_{dW}^{corrected} \leftarrow \frac{V_{dW}}{(1 - \beta_1^t)}
	&V_{db}^{corrected} \leftarrow \frac{V_{db}}{(1 - \beta_1^t)}\\
	&S_{dW}^{corrected} \leftarrow \frac{S_{dW}}{(1 - \beta_2^t)}
	&S_{db}^{corrected} \leftarrow \frac{S_{db}}{(1 - \beta_2^t)}\\
	&W \leftarrow W - \alpha \frac{V_{dW}^{corrected}}{\sqrt{S_{dW}^{corrected}} + \epsilon}
	& b \leftarrow b - \alpha \frac{V_{db}^{corrected}}{\sqrt{S_{db}^{corrected}} + \epsilon}
\end{align}

En este algoritmo tenemos cuatro hiperparámetros, $\beta_1$, $\beta_2$, $\alpha$ y $\epsilon$ \cite{kingma2017adam}. Los valores típicos de estos hiperparámetros son los siguientes:

\begin{align*}
	\beta_1 &= 0.9\\
	\beta_2 &= 0.999\\
	\epsilon &= 10^{-8}
\end{align*}

\subsection{Mejoras para el modelo de Red neuronal}

\subsubsection{Regularización}

La regularización sirve para evitar el riesgo de un sobreajuste de un modelo complejo a los datos de entrenamiento. El mecanismo es muy sencillo, simplemente se añade una penalización a la función de pérdida para que el modelo resultante sea menos propenso a \textit{overfitting}:

\begin{align*}
	&J(W,b;x,y) = J(W,b;x,y) + \lambda \sum_{j=1}^n W_j^2\\
\end{align*}

En este caso, $\lambda$ es un hiperparámetro que reprenta cuánto queremos penalizar la flexibilidad del modelo, por lo que también se elige usando el conjunto de validación. Si tomamos un valor muy alto para $\lambda$, los parámetros $W$ y $b$ serán muy próximos a 0 y tal vez genere un problema de alto sesgo. Por el contrario, un valor muy cercado a 0 apenas penalizaría el modelo y la regularización no tendría efecto. Si usamos el valor correcto de $\lambda$, pagaríamos con un poco de \textit{bias} para una mejoría mayor en la \textit{variability}. En cada paso del gradiente descendiente se realiza ahora lo siguiente:

\begin{align*}
	&\frac{\partial}{\partial W^{(l)}}_{ij} J(W,b) = \left[ \frac{\partial}{\partial W^{(l)_{ij}}} J(W,b;x^{(i)},y^{(i)}) \right] + 2 \lambda W^{(l)}_{ij}\\
	&\frac{\partial}{\partial b^{(l)}}_i J(W,b) = \frac{\partial}{\partial b^{(l)}_i} J(W,b;x^{(i)},y^{(i)})\\
\end{align*}

\subsubsection{Batch Normalization}

Gracias a este método podemos acelerar el aprendizaje del modelo y además generar redes neuronales más profundas y con mejores predicciones. Este método lo que hace normalizar las activaciones para acelerar el aprendizaje. Básicamente coge las varianza y la media para hacer que $\mu = 0$ y $\sigma = 1$:

\begin{align}
	&\mu \leftarrow \frac{1}{m} \sum_{i=1}^m z_i\\
	&\sigma^2 \leftarrow \frac{1}{m} \sum_{i=1}^m (z_i - \mu)^2\\
	&\hat{z_i} \leftarrow \frac{z_i - \mu}{\sqrt{\sigma ^ 2 + \epsilon}}\\
	&\hat{z_i} \leftarrow \gamma \hat{z_i} + \beta 
\end{align}

Los parámetros $\gamma$ y $\beta$ deben ser aprendidos. $\epsilon$ sirve para evitar la inestabilidad numérica debido a una división por 0, por tanto su valor debe ser muy pequeño \cite{DBLP:journals/corr/IoffeS15}.

\subsection{Autoencoders}

Los autoencoders son redes neuronales utilizadas, entre otras cosas, para reducir la dimensionalidad de los datos. A veces da mejores aprender de estos datos de dimensión reducida, es por eso que sería interesante probar este modelo. Estas redes neuronales se entrenan para reconstruir su \textit{input}, es por eso que están formadas por dos partes: un \textit{encoder} y un \textit{decoder}. El \textit{encoder} se encarga de reducir los datos de la entrada a una dimensión menor (al igual que el algoritmo PCA). Los datos comprimidos en menor dimensión reciben el nombre de código. La parte del \textit{decoder} se encarga de reconstruir los datos originales con la menor pérdida posible. Estas dos redes neuronales que forman el autoencoder se suelen entrenar juntas.

\begin{figure}[H]
	\includegraphics[scale=0.3]{imagenes/03_Estado_del_arte/autoencoder.png}
	\centering
	\caption{Ejemplo de autoencoder \cite{vae}.}
	\label{fig:autoencoder}
\end{figure}

Este problema, se suele definir formalmente como aprender las funciones $A: \mathbb{R}^n \rightarrow \mathbb{R}^p$ (encoder) y $B: \mathbb{R}^p \rightarrow \mathbb{R}^n$ (decoder) que satisfaga

\begin{align}
	&\argmin_{A,B}E[\Delta (x, B \circ A(x))]
\end{align}

Donde E es la media y $\Delta$ es la función que calcula el error de reconstrucción, que mide la distancia entre la salida del docoder y el input.\newline

Este tipo de modelos se suele usar mucho para clasificación o predicción de series temporales, para reducción de dimensionalidad, para crear Generative Adversarial Networks, etc. Existen varios tipos de autoencoders que serán explicados a continuación.

\subsubsection{Sparse Autoencoders}

Para evitar que tras el entrenamiento se obtengan A y B como funciones identidades, podemos forzar a que las activaciones de las capas ocultas sean dispersas. Para ello, podríamos usar regularización $L_1$, la que hemos visto antes. La diferencia es que la aplicaríamos sobre las activaciones en lugar de los pesos. Por tanto el objetivo sería

\begin{align}
	&\argmin_{A,B}E[\Delta (x, B \circ A(x))] + \lambda \sum_i |a_i|
\end{align}

Donde $a_i$ es el valor de la activación de la i-ésima capa oculta. Otra forma de hacerlo sería usando la divergencia de Kullback-Leibler (mide la distancia entre dos funciones de distribución) asumiendo que las activaciones siguen una distribución de Bernuilli con probabilidad $p$. La función de pérdida pasaría a ser

\begin{align}
	&\argmin_{A,B}E[\Delta (x, B \circ A(x))] + \sum_j KL(p||\hat{p}_j)\\
	&\hat{p}_j = \frac{1}{m} \sum_i a_j^{(2)}(x^{(i)})
\end{align}

Donde $a_i(x)$ representa el valor de la activación dada una entrada $x$. En este caso, el término de regularización es $p$, el cuál es forzado a aproximarse a $\hat{p}$. Normalmente $p$ suele ser un valor cercano a 0 \cite{ng, bank2020autoencoders}.

\subsubsection{Stacked Autoencoders}

Los autoencoders dispersos solo sirven para redes neuronales de tres capas, una para el encoder, otra para el código y otra para el decoder. Pero, ¿qué pasa si los datos que queremos aprender patrones más complejos en lugar de patrones lineales? La solución es simple, basta con añadir más capas al autoencoder en la parte del encoder, del decoder o en ambas \cite{bank2020autoencoders}.\newline

Las redes neuronales son un modelo muy potente y propensas a \textit{overfitting}, por eso la regularización es siempre esencial en estos casos.

\subsubsection{Varitional Autoencoders}

Este tipo de autoencoders trata de describir la generación de datos a través de una distribución de probabilidad. Por tanto, en lugar de mapear la entrada en un vector de tamaño fijo, la vamos a mapear a una distribución. Por consecuencia, se usarán dos vectores uno que representa la media y el otro la desviación típica. La función de pérdida para este tipo de Autoencoder es la siguiente \cite{bank2020autoencoders}:

\begin{align}
	&\mathcal{L}(\theta, \phi; x^{(i)}) = \mathbb{E}_{q_\phi(z|x^{(i)})} \left[ \log p_\theta(x^{(i)}|z) \right] - KL(q_\phi(z|x_i)||p_\theta(z))
\end{align}

En la anterior función, $\mathbb{E}_{q_\phi(z|x^{(i)})} \left[ \log p_\theta(x^{(i)}|z) \right]$ representa la función de pérdida (como por ejemplo el MSE que usamos antes en la sección sobre redes neuronales).\newline

Para ejecutar la propagación hacia detrás, $\mu$ (vector que representa la media de la distribución de los datos) y $\sigma$ (vector que representa la desviación típica de la distribución de los datos) se pasan a un único vector latente $z$ usando el truco de reparametrización. Los más usual es aproximar $p(z|x)$ a una distribución Gaussiana $q_\phi(z|x) = \mathcal{N}(g(x),h(x))$ donde $g(x)$ es la media y $h(x)$ es la covarianza de la distribución. Por tanto, podemos aplicar el truco de reparametrización esta manera \cite{bank2020autoencoders}:

\begin{align}
	&z = h(x) \xi + g(x)
\end{align}

Donde $\xi \sim \mathcal{N}(0,\mathbf{I})$ sigue una distribución normal. La KL divergencia en este caso se calcula de la siguiente manera \cite{kingma2014autoencoding}:

\begin{align}
	&KL(q_\phi(z|x_i)||p_\theta(z)) = \frac{1}{2} \sum^J_{j=1} \left( 1 + \log ((\sigma_j)^2) - (\mu_j)^2 - (\sigma_j)^2 \right)
\end{align}

\begin{figure}[H]
	\includegraphics[scale=0.3]{imagenes/03_Estado_del_arte/vae-gaussian.png}
	\centering
	\caption{El proceso del truco de reparametrización explicado gráficamente \cite{weng2018VAE}.}
	\label{fig:gaussianvae}
\end{figure}

\subsubsection{Autoencoder para clasificación}\label{subsub:classifierautoencoder}

Dado el error de de reconstrucción como $R(x,\hat{x})$ y el error de clasificación $\mathcal{L}(y,\hat{y})$, podemos usar la función de pérdida $\tilde{\mathcal{L}} = \mathcal{L}(y,\hat{y}) + \lambda R(x,\hat{x})$ para entrenar un autoencoder que clasifica a partir de las variables generadas en el espacio latente. En esta nueva función, el error de reconstrucción pasa a ser el valor de regularización.

\begin{figure}[H]
	\includegraphics[scale=0.3]{imagenes/03_Estado_del_arte/classifierautoencoder.png}
	\centering
	\caption{Representación gráfica de un Autoencoder clasificador \cite{bank2020autoencoders}.}
	\label{fig:classiferautoencoder}
\end{figure}

\subsection{Redes Neuronales Convolucionales}

Las redes neuronales convolucionales son un modelo muy ampliamente usado hoy en día para detección de patrones en imágenes (datos 2D) el cual es muy potente y da buenos resultados. En este trabajo estamos buscando patrones en un vector imágenes de un único canal. Este tipo de redes se puede aplicar también sobre 3D, pero no es el caso.\\

La idea básica de una capa convolucional consiste en un vector que llamaremos \textit{Kernel} que contiene los pesos e iremos moviendo a través del vector.\\

\begin{figure}[H]
	\includegraphics[scale=0.3]{imagenes/03_Estado_del_arte/conv2d.png}
	\centering
	\caption{Explicación gráfica de las Convoluciones Bidimensionales.}
	\label{fig:conv1d}
\end{figure}

Cada red de la arquitectura convolucional hace que el vector vaya perdiendo tamaño a la vez que vamos detectando patrones en el mismo. Por ejemplo, un vector de tamaño 6 usando un \textit{Kernel} de tamaño 3 quedaría en un vector de tamaño 4. Aunque con \textit{padding} esto se puede evitar (viene explicado más adelante).\\

Con este tipo de capas, también podríamos diseñar un autoencoder y estaría muy interesante ver si es capaz de competir con el Autoencoder diseñado con una red neuronal totalmente conectada. De hecho, los autoencoders convolucionales suelen usarse mucho y tienen un gran potencial en segmentación de imágenes y como GANs. Un ejemplo de uso es en U-NET, una CNN diseñada para segmentación de imágenes médicas \cite{DBLP:journals/corr/RonnebergerFB15}.\\

Este tipo de modelo también puede combinarse con redes neuronales totalmente conectadas para realizar la clasificación, por lo que también estaría interesante comparar el potencial entre simplemente usar únicamente capas totalmente conectadas o usar una combinación.\\

Una capa de convolución puede tener más de un \textit{Kernel}, así que podemos tener de salida dos vectores o más lo cual puede llegar a detectar más patrones de los datos de entrada.

\subsubsection{Modificaciones en las convoluciones}

Una de las modificaciones más importantes es el \textit{padding}. Esta modificación permite que al realizar una convolución no se reduzca el tamaño del vector, permitiéndonos entrenar redes convolucionales más profundas. Esto se consigue añadiendo 0s a los extremos del vector.\\

Otra modificación interesante es el \textit{stride}, la cual solo consiste en saltarse entre convolución y convolución $n$ posiciones a la derecha.

\subsubsection{Pooling layers}

Algunas arquitecturas de redes convolucionales, además de usar capas convolucionales y totalmente conectadas, también usan \textit{pooling layers}. Este tipo de capas se utilizan para acelerar la computación, pues no tienen parámetros que aprender. Los dos tipos principales de \textit{pooling} son:

\begin{itemize}
	\item \textbf{Max pooling}: consiste en un \textit{kernel} que va a ir tomando el máximo valor de la región del vector (o dato que estemos procesando) en la que se encuentra.
	\item \textbf{Average pooling}: en lugar de tomar el máximo valor, tomaremos el valor medio.
\end{itemize}

\subsubsection{Convolutional Autoencoders}\label{subsub:convautoencoder}

Este tipo de modelos hace uso de varias capas convolucionales en el \textit{encoder} para ir reduciendo las dimensiones de la imagen a la vez que aumentamos el número de canales. Después, las activaciones de la última capa convolucional se convierten en un vector unidimensional para posteriormente reducirle la dimensionalidad usando una capa totalmente concectada y obtener el código latente.\\

Para el \textit{decoder}, realizamos la operación inversa a la que hicimos en el \textit{encoder}. Empezamos aumentado la dimensionalidad del código del espacio latente con una capa totalmente conectada y la transformamos en vector tridimensional (de las dimensiones que teníamos antes de convertirlo a un vector unidimensional) y a partir de ahí aplicamos deconvoluciones hasta llegar a una imagen con tamaño similar a la original.\\

Pero, ¿cómo se podría usar este tipo de redes para clasificación? Para esto, podríamos entrenar un clasificador (usando red totalmente conectada) del espacio latente a la vez que entrenamos el autoencoder. El procedimiento usado sería similar al explicado en la Subsección \ref{subsub:classifierautoencoder}, usando como función de pérdida para la clasificación la Cross Entropy y como error de reconstrucción el MSE (ambos explicados en la Subsección \ref{subsub:gradientdescent}).

\begin{figure}[H]
	\includegraphics[scale=0.3]{imagenes/05_Implementacion/CAE.png}
	\centering
	\caption{Ejemplo de Convolutional Autoencoder. \cite{cae}}
	\label{fig:cae}
\end{figure}

\subsubsection{ResNets}

Las \textit{Residual Networks} son una arquitectura de Red Convolucional muy potente que nos permite entrenar redes muy profundas. Un \textit{ResNet block} consta de dos o tres capas convolucionales seguidas de una suma de las activaciones de la capa anterior, tal y como se muestra en la Figura \ref{fig:resnet}. Concatenando varios \textit{Resnet Blocks} podemos crear una \textit{ResNet}. Más adelante, en la Subsección \ref{subsec:resnetarch} se explica más sobre esta arquitectura.

\begin{figure}[H]
	\includegraphics[scale=0.35]{imagenes/03_Estado_del_arte/resnet-block.png}
	\centering
	\caption{Ejemplos de \textit{Resnet Blocks} \cite{resnetblocks}.}
	\label{fig:resnetblocks}
\end{figure}
